package org.me.CoViKoa;

import org.apache.jena.geosparql.configuration.GeoSPARQLConfig;
import org.apache.jena.geosparql.configuration.GeoSPARQLOperations;
import org.apache.jena.ontology.*;
import org.apache.jena.query.*;
import org.apache.jena.rdf.model.*;
import org.apache.jena.reasoner.ValidityReport;
import org.apache.jena.sparql.expr.aggregate.AggregateRegistry;
import org.apache.jena.sparql.function.FunctionRegistry;
import org.apache.jena.sparql.graph.NodeConst;
import org.apache.jena.sparql.pfunction.PropertyFunctionRegistry;
import org.apache.jena.update.*;
import org.apache.jena.util.FileUtils;
import org.apache.jena.vocabulary.RDF;
import org.me.CoViKoa.AdditionalGeoSparqlFunctions.AggregateGeomFunction;
import org.me.CoViKoa.AdditionalGeoSparqlFunctions.Centroid;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;
import org.topbraid.jenax.util.JenaUtil;
import org.topbraid.shacl.rules.RuleUtil;

import java.io.*;
import java.util.*;
import java.util.regex.Matcher;
import java.util.regex.Pattern;

/**
 * Class allowing to load project ontologies, the derivation model and to instrument the CoViKoa reasoning
 * (using SHACL internally).
 *
 * @author Matthieu Viry
 * @version 0.2.0
 */
public class CoViKoaHandler {
    private final static Logger logger = LoggerFactory.getLogger(CoViKoaHandler.class);
    /**
     * Data model on which we will work.
     *
     * This is the ontological model containing the base individuals and on which
     * the new individuals will be inserted.
     *
     */
    private final OntModel dataModel;
    /**
     * OntModel containing the covikoa base ontologies
     * (covikoa-geoviz, covikoa-interaction, etc.)
     */
    private final OntModel baseOnto;
    /**
     * Model containing SHACL shapes and rules.
     */
    private final Model shapesModel;
    /**
     * OntModel hosting the new triples generated by SHACL rules.
     */
    private OntModel newTriples = ModelFactory.createOntologyModel(OntModelSpec.OWL_MEM_MICRO_RULE_INF);
    /**
     * Model containing the union of the dataModel and the newTriples.
     */
    private Model unionModel = null;
    /**
     * The last validation report produced.
     *
     * SHACL validation is triggered each time triples are inserted in the data model.
     * This variable contains the last validation report produced.
     */
    private Model lastValidationReport;

    /**
     * The URI of the triple acting as a marker to determine the current rule execution set.
     */
    private String uriMarkerRuleExecutionMarker = null;
    private final Boolean applyGeoSpaqlInferencing;

    public CoViKoaHandler(String[] vocabulariesFile, String[] dataFiles, String derivationFile, Boolean applyGeoSpaqlInferencing) throws FileNotFoundException {
        // Setup :
        //   - initialize the ARQ engine
        //   - register our custom geo aggregation functions in the AggregateRegistry
        //   - register our custom geo functions in the FunctionRegistry
        //   - setup GeoSPARQL functionalities / spatial index
        ARQ.init();
        AggregateRegistry.register("http://lig-tdcge.imag.fr/steamer/covikoa/geo-agg#Union", AggregateGeomFunction.unionAccumulatorFactory, NodeConst.nodeMinusOne);
        AggregateRegistry.register("http://lig-tdcge.imag.fr/steamer/covikoa/geo-agg#Intersection", AggregateGeomFunction.intersectionAccumulatorFactory, NodeConst.nodeMinusOne);
        AggregateRegistry.register("http://lig-tdcge.imag.fr/steamer/covikoa/geo-agg#Extent", AggregateGeomFunction.envelopeAccumulatorFactory, NodeConst.nodeMinusOne);
        FunctionRegistry.get().put("http://lig-tdcge.imag.fr/steamer/covikoa/geof#Centroid", Centroid.class);
        PropertyFunctionRegistry.chooseRegistry(ARQ.getContext())
                .put("http://lig-tdcge.imag.fr/steamer/covikoa/geoviz#asGeoStylerJSON", new SymboliserSerialisationPropertyFunction());
        PropertyFunctionRegistry.chooseRegistry(ARQ.getContext())
                .put("http://lig-tdcge.imag.fr/steamer/covikoa/interaction#interactionAsJSON", new InteractionSerialisationPropertyFunction());
        GeoSPARQLConfig.setupMemoryIndex();
        this.applyGeoSpaqlInferencing = applyGeoSpaqlInferencing;

        // Read the files containing the covikoa ontologies
        this.baseOnto = ModelFactory.createOntologyModel(OntModelSpec.OWL_MEM_MICRO_RULE_INF);
        if (vocabulariesFile != null) {
            for (String dataFilePath : vocabulariesFile) {
                baseOnto.read(new FileInputStream(dataFilePath), null, FileUtils.langTurtle);
            }
        }

        // Read the files containing the ontology/ies targeted by covikoa and the possible individuals
        this.dataModel = ModelFactory.createOntologyModel(OntModelSpec.OWL_MEM_MICRO_RULE_INF);
        if (dataFiles != null) {
            for (String dataFilePath : dataFiles) {
                dataModel.read(new FileInputStream(dataFilePath), null, FileUtils.langTurtle);
            }
        }

        // Validate the logical consistency of the Semantic Data Model and its data
        this.validateModel();

        // We want to reuse the derivation model and the generated rules from this object...
        RulesGenerator rgen = new RulesGenerator(derivationFile);
        dataModel.add(rgen.derivationModel);
        this.shapesModel = rgen.generatedRules;

        // We need the inferences that can be drawn from our set of base ontologies
        // both on the data model and on the future new triples from the shacl reasoning
        dataModel.add(baseOnto);
        newTriples.add(baseOnto);

        // Now that everything is loaded we can start a first run of the inference system...
        this.recursiveInferFromRulesAndUpdateModel();
    }

    private void createRuleExecutionIndiv() {
        // Retrieve the current datetime
        Calendar cal = GregorianCalendar.getInstance();
        Literal dtValue = dataModel.createTypedLiteral(cal);

        // Create the RuleExecutionMarker (we keep track of its URI because we will delete it later)
        this.uriMarkerRuleExecutionMarker = "urn:id:RuleExecutionMarker-" + UUID.randomUUID();
        Resource resource = dataModel.createResource( this.uriMarkerRuleExecutionMarker );
        Resource ruleExecMarker = dataModel.getResource("http://lig-tdcge.imag.fr/steamer/covikoa/derivation#RuleExecutionMarker");
        Property p = dataModel.getProperty("http://www.w3.org/2006/time#inXSDDateTime" );
        dataModel.add(dataModel.createStatement(resource, RDF.type, ruleExecMarker));
        dataModel.add(dataModel.createStatement(resource, p, dtValue));

        // Create a prov:Generation (it wont be deleted later so we can keep track of
        // the number of generations + knowing during rule execution that some knowledge
        // was infered during a previous generation).
        Resource thisgen = dataModel.createResource("urn:id:Generation-" + UUID.randomUUID());
        Resource ruleExecActivity = dataModel.getResource("http://lig-tdcge.imag.fr/steamer/covikoa/derivation#RuleExecution");
        Resource prov_gen = dataModel.getResource("http://www.w3.org/ns/prov#Generation");
        Property prov_attime = dataModel.getProperty("http://www.w3.org/ns/prov#atTime");
        Property prov_activity = dataModel.getProperty("http://www.w3.org/ns/prov#activity");
        dataModel.add(dataModel.createStatement(thisgen, RDF.type, prov_gen));
        dataModel.add(dataModel.createStatement(thisgen, prov_attime, dtValue));
        dataModel.add(dataModel.createStatement(thisgen, prov_activity, ruleExecActivity));
    }

    private void deleteRuleExecutionIndiv() {
        if (uriMarkerRuleExecutionMarker != null) {
            Individual r = dataModel.getIndividual(uriMarkerRuleExecutionMarker);
            dataModel.removeAll(r, null, (RDFNode) null);
        }
    }

    public void recursiveInferFromRulesAndUpdateModel() {
        // We want to keep track of the number of iterations necessary to have produced all the triplets
        // (the inference "loop" will only stop after the last pass didn't produce any triple).
        // Note: There is a lot of noise for timing operations and preparing log messages in this function,
        // to be improved.
        long startTime;
        long estimatedTime;
        long n_iter = 0;
        String logmsg;
        long cumultime = 0;
        long cumulnewtriples = 0;

        if (unionModel == null) {
            unionNewTriplesDataModel();
        }

        // Create the RuleExecutionMarker and the Generation individuals
        this.createRuleExecutionIndiv();
        // Run the inference loop
        while (true) {
            logmsg = "";
            n_iter += 1;
            startTime = System.currentTimeMillis();
            // Fetch the inference graph according to our SHACL rules
            long nbInferedTriples = this.inferFromRules();
            estimatedTime = System.currentTimeMillis() - startTime;
            logmsg += "Executing SHACL rules ...\n\tSize of newTriples: " + nbInferedTriples
                    + "\n\tInference: " + estimatedTime + "ms";
            cumultime += estimatedTime;

            if (nbInferedTriples > 0) {
                // If any new triples, add them to the data model
                // after having applied GeoSPARQL inferencing on them if needed ...
                if (applyGeoSpaqlInferencing) {
                    startTime = System.currentTimeMillis();
                    GeoSPARQLOperations.applyInferencing(this.newTriples);
                    estimatedTime = System.currentTimeMillis() - startTime;
                    logmsg += "\n\tApplying GeoSPARQL inferencing: " + estimatedTime + "ms";
                    cumultime += estimatedTime;
                }
                cumulnewtriples += nbInferedTriples;
                startTime = System.currentTimeMillis();
                this.unionNewTriplesDataModel();
                estimatedTime = System.currentTimeMillis() - startTime;
                cumultime += estimatedTime;
                logmsg += "\n\tUnion new triples and DataModel: " + estimatedTime + "ms";
                logger.info(logmsg);
            } else {
                // ... otherwise, we are done here.
                logger.info(logmsg);
                break;
            }
        }
        // Delete the RuleExecutionMarker (the Generation individual stays in the graph)
        this.deleteRuleExecutionIndiv();
        logger.info("Exited recursiveInferFromRulesAndUpdateModel after " + n_iter + " iterations (took a total of " + cumultime + "ms for " + cumulnewtriples + " new triples).");
    }

    public long inferFromRules(){
        Model temp = JenaUtil.createDefaultModel();
        RuleUtil.executeRules(unionModel, shapesModel, temp, null);
        newTriples.add(temp);
        return temp.size();
    }

    private void unionNewTriplesDataModel() {
        unionModel = ModelFactory.createUnion(dataModel, newTriples);
    }

    public Model getNewTriples() {
        return newTriples;
    }

    public Model getDataModel() {
        return dataModel;
    }

    public Model getUnionModel() {
        return unionModel;
    }

    public Model getShapesModel() {
        return shapesModel;
    }

    public Model getLastValidationReport() {
        return lastValidationReport;
    }

    private ResultSet queryModel(Model model, String queryString) throws QueryException {
        Query query = QueryFactory.create(queryString);
        QueryExecution qexec = QueryExecutionFactory.create(query, model);
        ResultSet results = qexec.execSelect();
        return results;
    }

    private String queryModelJSON(Model model, String queryString) {
        try {
            ResultSet results = queryModel(model, queryString);
            ByteArrayOutputStream stringWriter = new ByteArrayOutputStream();
            ResultSetFormatter.outputAsJSON(stringWriter, results);
            return stringWriter.toString();
        } catch (QueryException e) {
            e.printStackTrace();
            return "{\"response\":\"Error\",\"message\":\"Unexpected error while querying the DataModel\"}";
        }
    }

    private List<String> queryModelToList(Model model, String queryString) {
        List<String> qLineResult = new ArrayList<>();
        try {
            ResultSet results = queryModel(model, queryString);
            while (results.hasNext())
            {
                QuerySolution soln = results.nextSolution() ;
                System.out.println(soln.toString());
                qLineResult.add(soln.toString());
            }
        } catch (QueryException e) {
            e.printStackTrace();
        }
        return qLineResult;
    }

    private String queryModelTurtleString(Model model, String queryString, String typeExec) {
        try {
            QueryExecution qexec = QueryExecutionFactory.create(queryString, model);
            Model resultModel = null;
            if (typeExec == "describe") {
                resultModel = qexec.execDescribe();
            } else if (typeExec == "construct") {
                resultModel = qexec.execConstruct();
            }
            StringWriter stringWriter = new StringWriter();
            resultModel.write(stringWriter, FileUtils.langTurtle);
            return stringWriter.toString();
        } catch (QueryException e) {
            e.printStackTrace();
            return "{\"response\":\"Error\",\"message\":\"Unexpected error while querying the DataModel\"}";
        }
    }

    public String queryUnionModelDescribe(String queryString) {
        return queryModelTurtleString(this.unionModel, queryString, "describe");
    }

    public String queryUnionModelConstruct(String queryString) {
        return queryModelTurtleString(this.unionModel, queryString, "construct");
    }

    public String queryDataModelDescribe(String queryString) {
        return queryModelTurtleString(this.dataModel, queryString, "describe");
    }

    public String queryDataModelConstruct(String queryString) {
        return queryModelTurtleString(this.dataModel, queryString, "construct");
    }

    public String writeUnionModel() {
        StringWriter stringWriter = new StringWriter();
        this.unionModel.write(stringWriter, FileUtils.langTurtle);
        return stringWriter.toString();
    }

    public String queryUnionModelJSON(String queryString) {
        return queryModelJSON(this.unionModel, queryString);
    }

    public String queryDataModelJSON(String queryString) {
        return queryModelJSON(this.dataModel, queryString);
    }

    public String queryNewTriplesJSON(String queryString) {
        return queryModelJSON(this.newTriples, queryString);
    }

    private String updateModel(Model model, String updateString) {
        try {
            UpdateAction.parseExecute(updateString, model);
        } catch (UpdateException e) {
            e.printStackTrace();
            return "{\"response\":\"Error\",\"message\":\"Unexpected error while updating the DataModel\"}";
        }
        recursiveInferFromRulesAndUpdateModel();
        return "{\"response\":\"OK\"}";
    }

    public String updateDataModel(String queryString) {
        // This is the update function called by the python server,
        // it has an effect on `dataModel` (and not on `unionModel`).
        // Because the query can delete information from `dataModel` (thus invalidating information in `newTriples`)
        // we clean the newTriples model as well as the unionModel if it is necessary
        Pattern pattern = Pattern.compile("delete", Pattern.CASE_INSENSITIVE);
        Matcher match = pattern.matcher(queryString);

        if (match.find()) {
            // There is a delete clause, so we restore newTriples and unionModel in their initial state
            this.unionModel = null;
            this.newTriples = ModelFactory.createOntologyModel(OntModelSpec.OWL_MEM_MICRO_RULE_INF);
            newTriples.add(baseOnto);
        }
        // Run the update and return the JSON response
        return updateModel(this.dataModel, queryString);
    }

    public void validateModel() {
        ValidityReport validity = this.dataModel.validate();
        if (validity.isValid()) {
            logger.warn("Validity data model = OK");
        } else {
            logger.warn("Validity data model = Conflicts");
            for (Iterator i = validity.getReports(); i.hasNext(); ) {
                logger.warn(" - " + i.next());
            }
        }
    }
}
